{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b42Qnv7p3gG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import Ridge, Lasso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/merged_data_2024_column.csv\" # CHANGE TO NEW FILE PATH\n",
        "df = pd.read_csv(file_path)\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7i8cZPrqBEi",
        "outputId": "7b3bb85a-f74d-4bf7-ff48-8d068b2abc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-afbfa55173df>:2: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "668892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns= [\"sellingprice\", \"zipcode\", \"sale_year\"], inplace= True)"
      ],
      "metadata": {
        "id": "UNDRa_nIqGGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))\n",
        "missing_summary_df1 = pd.DataFrame({\n",
        "    \"Missing Values\": df.isnull().sum(),\n",
        "    \"Missing Percentage (%)\": (df.isnull().sum() / len(df)) * 100\n",
        "})\n",
        "print(missing_summary_df1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B165nsV9qQ-c",
        "outputId": "3c8ab3b0-4b47-477e-a0e2-40d593be72a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "668892\n",
            "            Missing Values  Missing Percentage (%)\n",
            "year                     0                     0.0\n",
            "make                     0                     0.0\n",
            "model                    0                     0.0\n",
            "trim                     0                     0.0\n",
            "state                    0                     0.0\n",
            "condition                0                     0.0\n",
            "odometer                 0                     0.0\n",
            "color                    0                     0.0\n",
            "interior                 0                     0.0\n",
            "mmr                      0                     0.0\n",
            "2024_price               0                     0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Whole Dataset LightGBM"
      ],
      "metadata": {
        "id": "LKwZsvBwSisR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['make', 'model', 'trim', 'state', 'color', 'interior']\n",
        "numerical_cols = ['year', 'condition', 'odometer', 'mmr']\n",
        "cluster_cols = ['year', 'condition', 'odometer', 'mmr']"
      ],
      "metadata": {
        "id": "DhDp0XT5u3Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = df.drop('2024_price', axis=1)\n",
        "y = df['2024_price']\n",
        "\n",
        "\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "X_numerical = X[numerical_cols].values\n",
        "X_combined = np.hstack((X[categorical_cols].values, X_numerical))\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2)\n",
        "\n",
        "\n",
        "model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "rmse = root_mean_squared_error(y_test, y_pred)\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afN5n19wDF8g",
        "outputId": "65a6d702-0fac-4b17-9253-42dd4b09c9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053747 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1446\n",
            "[LightGBM] [Info] Number of data points in the train set: 535113, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 17325.880509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error: 5483.754146773576\n",
            "R-squared: 0.8439104853320114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering"
      ],
      "metadata": {
        "id": "szbh1BDkSoX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only cluster on numeric features and exclude target\n",
        "numerical_features = df[numerical_cols]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(numerical_features)"
      ],
      "metadata": {
        "id": "5UpzJwBkwGBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state= 42)\n",
        "kmeans.fit(scaled_features)\n",
        "cluster_labels = kmeans.labels_\n",
        "df['cluster'] = cluster_labels"
      ],
      "metadata": {
        "id": "lzvUzT2OznV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df[df[\"cluster\"]==0]))\n",
        "print(len(df[df[\"cluster\"]==1]))\n",
        "print(len(df[df[\"cluster\"]==2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4CgObqYb_Qb",
        "outputId": "2325125a-65fa-4fe9-f1e5-54f8c438d137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "136622\n",
            "461080\n",
            "71190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression on Clusters"
      ],
      "metadata": {
        "id": "dLxKDYLOU5ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_results = {}\n",
        "\n",
        "for cluster in df[\"cluster\"].unique():\n",
        "  X = df[df[\"cluster\"] == cluster].drop(['2024_price', \"cluster\"], axis=1)\n",
        "\n",
        "  y = df[df[\"cluster\"] == cluster]['2024_price']\n",
        "\n",
        "  X_numerical = X[numerical_cols]\n",
        "  scaler = StandardScaler()\n",
        "  scaled_features = scaler.fit_transform(X_numerical)\n",
        "\n",
        "  label_encoders = {}\n",
        "  for col in categorical_cols:\n",
        "      le = LabelEncoder()\n",
        "      X[col] = le.fit_transform(X[col].astype(str))\n",
        "      label_encoders[col] = le\n",
        "\n",
        "\n",
        "\n",
        "  X_combined = np.hstack((X[categorical_cols].values, scaled_features))\n",
        "  # print(X_combined[5])\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2)\n",
        "\n",
        "  print(f\"-----Training Models for Cluster {cluster}-----\")\n",
        "  ridge_model = Ridge(alpha= 10)\n",
        "  lasso_model = Lasso()\n",
        "\n",
        "  ridge_model.fit(X_train, y_train)\n",
        "  lasso_model.fit(X_train, y_train)\n",
        "\n",
        "  ridge_y_pred = ridge_model.predict(X_test)\n",
        "  lasso_y_pred = lasso_model.predict(X_test)\n",
        "\n",
        "  ridge_rmse = root_mean_squared_error(y_test, ridge_y_pred)\n",
        "  lasso_rmse = root_mean_squared_error(y_test, lasso_y_pred)\n",
        "\n",
        "  ridge_r2 = r2_score(y_test, ridge_y_pred)\n",
        "  lasso_r2 = r2_score(y_test, lasso_y_pred)\n",
        "\n",
        "  regression_results[cluster] = {\n",
        "      \"Ridge\": {\"model\": ridge_model, \"RMSE\": ridge_rmse, \"R2\": ridge_r2},\n",
        "      \"Lasso\": {\"model\": lasso_model, \"RMSE\": lasso_rmse, \"R2\": lasso_r2}\n",
        "  }\n",
        "  print(f\"Training Complete for Cluster {cluster}\\n\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLcbZaT1U8R6",
        "outputId": "27004008-7c60-4a7e-94d8-31d24f26efe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Training Models for Cluster 2-----\n",
            "Training Complete for Cluster 2\n",
            "\n",
            "\n",
            "-----Training Models for Cluster 0-----\n",
            "Training Complete for Cluster 0\n",
            "\n",
            "\n",
            "-----Training Models for Cluster 1-----\n",
            "Training Complete for Cluster 1\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regression_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckr3PQXGWxoY",
        "outputId": "a17c30b5-00b3-482f-d999-b75fdf2bedc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{np.int32(2): {'Ridge': {'model': Ridge(alpha=10),\n",
              "   'RMSE': 3200.8281800840596,\n",
              "   'R2': 0.9347511461178278},\n",
              "  'Lasso': {'model': Lasso(),\n",
              "   'RMSE': 3200.8558364385767,\n",
              "   'R2': 0.9347500185638766}},\n",
              " np.int32(0): {'Ridge': {'model': Ridge(alpha=10),\n",
              "   'RMSE': 2757.4445278888893,\n",
              "   'R2': 0.9522187888515746},\n",
              "  'Lasso': {'model': Lasso(),\n",
              "   'RMSE': 2757.468358403865,\n",
              "   'R2': 0.9522179629740083}},\n",
              " np.int32(1): {'Ridge': {'model': Ridge(alpha=10),\n",
              "   'RMSE': 8816.669493123107,\n",
              "   'R2': 0.25763726713395974},\n",
              "  'Lasso': {'model': Lasso(),\n",
              "   'RMSE': 8816.672036644964,\n",
              "   'R2': 0.2576368388053011}}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM on Clusters"
      ],
      "metadata": {
        "id": "D93OcHdHU2QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lightgbm_results = {}\n",
        "for cluster in df[\"cluster\"].unique():\n",
        "  X = df[df[\"cluster\"] == cluster].drop(['2024_price', \"cluster\"], axis=1)\n",
        "  y = df[df[\"cluster\"] == cluster]['2024_price']\n",
        "\n",
        "\n",
        "  X_numerical = X[numerical_cols]\n",
        "  scaler = StandardScaler()\n",
        "  scaled_features = scaler.fit_transform(X_numerical)\n",
        "\n",
        "  label_encoders = {}\n",
        "  for col in categorical_cols:\n",
        "      le = LabelEncoder()\n",
        "      X[col] = le.fit_transform(X[col].astype(str))\n",
        "      label_encoders[col] = le\n",
        "\n",
        "  X_combined = np.hstack((X[categorical_cols].values, scaled_features))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "  model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)  # Adjust hyperparameters as needed\n",
        "  print(f\"-----Training LightGBM for cluster {cluster}-----\")\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  rmse = root_mean_squared_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  lightgbm_results[cluster] = {\"model\": model, \"RMSE\": rmse, \"R2\": r2}\n",
        "  print(\"Training Complete\")\n",
        "  print(f\"Root Mean Squared Error: {rmse}\")\n",
        "  print(f\"R-squared: {r2}\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymh1_BNBS-CH",
        "outputId": "49203161-083c-4690-ee0a-cc7d9934501d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Training LightGBM for cluster 2-----\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001859 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1190\n",
            "[LightGBM] [Info] Number of data points in the train set: 56952, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 16883.227155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Complete\n",
            "Root Mean Squared Error: 2186.8120110103027\n",
            "R-squared: 0.9701892772492449\n",
            "\n",
            "\n",
            "-----Training LightGBM for cluster 0-----\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017333 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1154\n",
            "[LightGBM] [Info] Number of data points in the train set: 109297, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 33555.216547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Complete\n",
            "Root Mean Squared Error: 2739.100704035502\n",
            "R-squared: 0.9540821124453215\n",
            "\n",
            "\n",
            "-----Training LightGBM for cluster 1-----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036392 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1391\n",
            "[LightGBM] [Info] Number of data points in the train set: 368864, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 12572.237023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Complete\n",
            "Root Mean Squared Error: 5959.27681733395\n",
            "R-squared: 0.6634219784493796\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning (Abandoned for Simpler Methods)"
      ],
      "metadata": {
        "id": "j51PKN6xR28w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "2tR1we3QR4Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CarPricePredictor(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(CarPricePredictor, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    self.nonlin = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.nonlin(self.fc1(x))\n",
        "    x = self.nonlin(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "TrJNhZVFSOL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse_loss(y_true, y_pred):\n",
        "  return torch.sqrt(torch.mean((y_true - y_pred)**2))\n"
      ],
      "metadata": {
        "id": "t2-UR0d8X74D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train, X_test, y_test, lr, epochs, batch_size, input_size):\n",
        "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "  y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "  y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "  train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = CarPricePredictor(input_size)\n",
        "  criterion = rmse_loss\n",
        "  optimizer = optim.Adam(model.parameters(), lr= lr)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      pred = model(batch_X)\n",
        "      loss = criterion(batch_y, pred)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_predictions = model(X_test_tensor)\n",
        "        test_rmse = criterion(test_predictions, y_test_tensor).item()\n",
        "\n",
        "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "QxExt9xCYB7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data for cluster 1\n",
        "X = df[df[\"cluster\"] == 1].drop(columns=['2024_price'])  # Drop target column\n",
        "y = df[df[\"cluster\"] == 1]['2024_price'].values  # Convert to NumPy array\n",
        "\n",
        "\n",
        "categorical_cols = ['make', 'model', 'trim', 'state', 'color', 'interior']\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "\n",
        "numerical_cols = ['year', 'condition', 'odometer', 'mmr', 'sale_year']\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_numerical = X[numerical_cols].values\n",
        "X_categorical = X[categorical_cols].values\n",
        "\n",
        "\n",
        "X_combined = np.hstack((X_categorical, X_numerical))\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure all data is NumPy before passing to train_model\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "# Train model\n",
        "input_size = X_train.shape[1]  # Number of features\n",
        "model = train_model(X_train, y_train, X_test, y_test, lr=0.01, epochs=50, batch_size=32, input_size=input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBlEpXeLY9Hz",
        "outputId": "1fc4ac64-44cb-4449-9068-991222695cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Epoch 0, Loss: 119094681.7734375\n",
            "Test RMSE: 21795.7930\n",
            "Epoch 1\n",
            "Epoch 1, Loss: 62013609.6484375\n",
            "Test RMSE: 21777.4082\n",
            "Epoch 2\n",
            "Epoch 2, Loss: 61946367.236328125\n",
            "Test RMSE: 21759.1621\n",
            "Epoch 3\n",
            "Epoch 3, Loss: 61868199.47705078\n",
            "Test RMSE: 21740.9746\n",
            "Epoch 4\n",
            "Epoch 4, Loss: 61834712.08300781\n",
            "Test RMSE: 21722.7988\n",
            "Epoch 5\n",
            "Epoch 5, Loss: 61730813.41455078\n",
            "Test RMSE: 21704.6445\n",
            "Epoch 6\n",
            "Epoch 6, Loss: 61732150.302246094\n",
            "Test RMSE: 21686.5332\n",
            "Epoch 7\n",
            "Epoch 7, Loss: 61671884.5078125\n",
            "Test RMSE: 21668.4551\n",
            "Epoch 8\n",
            "Epoch 8, Loss: 61638710.03417969\n",
            "Test RMSE: 21650.3945\n",
            "Epoch 9\n",
            "Epoch 9, Loss: 61481923.829589844\n",
            "Test RMSE: 21632.3555\n",
            "Epoch 10\n",
            "Epoch 10, Loss: 61394805.53613281\n",
            "Test RMSE: 21614.3379\n",
            "Epoch 11\n",
            "Epoch 11, Loss: 61366397.951171875\n",
            "Test RMSE: 21596.3496\n",
            "Epoch 12\n",
            "Epoch 12, Loss: 61333066.13330078\n",
            "Test RMSE: 21578.3750\n",
            "Epoch 13\n",
            "Epoch 13, Loss: 61269964.92285156\n",
            "Test RMSE: 21560.4785\n",
            "Epoch 14\n",
            "Epoch 14, Loss: 61222385.078125\n",
            "Test RMSE: 21542.5586\n",
            "Epoch 15\n",
            "Epoch 15, Loss: 61148110.114746094\n",
            "Test RMSE: 21524.7168\n",
            "Epoch 16\n",
            "Epoch 16, Loss: 61099146.119140625\n",
            "Test RMSE: 21506.8535\n",
            "Epoch 17\n",
            "Epoch 17, Loss: 61053433.38964844\n",
            "Test RMSE: 21489.0312\n",
            "Epoch 18\n",
            "Epoch 18, Loss: 60885039.50048828\n",
            "Test RMSE: 21471.2480\n",
            "Epoch 19\n",
            "Epoch 19, Loss: 60866987.829589844\n",
            "Test RMSE: 21453.4551\n",
            "Epoch 20\n",
            "Epoch 20, Loss: 60841504.958984375\n",
            "Test RMSE: 21435.7344\n",
            "Epoch 21\n",
            "Epoch 21, Loss: 60842245.78417969\n",
            "Test RMSE: 21418.0508\n",
            "Epoch 22\n",
            "Epoch 22, Loss: 60779360.9921875\n",
            "Test RMSE: 21400.3711\n",
            "Epoch 23\n",
            "Epoch 23, Loss: 60555804.67675781\n",
            "Test RMSE: 21382.6875\n",
            "Epoch 24\n",
            "Epoch 24, Loss: 60586864.778808594\n",
            "Test RMSE: 21365.0840\n",
            "Epoch 25\n",
            "Epoch 25, Loss: 60586994.830566406\n",
            "Test RMSE: 21347.4961\n",
            "Epoch 26\n",
            "Epoch 26, Loss: 60494831.68310547\n",
            "Test RMSE: 21329.9336\n",
            "Epoch 27\n",
            "Epoch 27, Loss: 60382906.39550781\n",
            "Test RMSE: 21312.3887\n",
            "Epoch 28\n",
            "Epoch 28, Loss: 60404087.18701172\n",
            "Test RMSE: 21294.8984\n",
            "Epoch 29\n",
            "Epoch 29, Loss: 60319251.182128906\n",
            "Test RMSE: 21277.4043\n",
            "Epoch 30\n",
            "Epoch 30, Loss: 60291265.28515625\n",
            "Test RMSE: 21259.9551\n",
            "Epoch 31\n",
            "Epoch 31, Loss: 60229442.71142578\n",
            "Test RMSE: 21242.5469\n",
            "Epoch 32\n",
            "Epoch 32, Loss: 60100849.889160156\n",
            "Test RMSE: 21225.1406\n",
            "Epoch 33\n",
            "Epoch 33, Loss: 60078630.072753906\n",
            "Test RMSE: 21207.7773\n",
            "Epoch 34\n",
            "Epoch 34, Loss: 59984871.157714844\n",
            "Test RMSE: 21190.4375\n",
            "Epoch 35\n",
            "Epoch 35, Loss: 59916668.63232422\n",
            "Test RMSE: 21173.1348\n",
            "Epoch 36\n",
            "Epoch 36, Loss: 59888955.518066406\n",
            "Test RMSE: 21155.8535\n",
            "Epoch 37\n",
            "Epoch 37, Loss: 59805359.818847656\n",
            "Test RMSE: 21138.5977\n",
            "Epoch 38\n",
            "Epoch 38, Loss: 59738056.341308594\n",
            "Test RMSE: 21121.3984\n",
            "Epoch 39\n",
            "Epoch 39, Loss: 59709901.97949219\n",
            "Test RMSE: 21104.2207\n",
            "Epoch 40\n",
            "Epoch 40, Loss: 59670341.498046875\n",
            "Test RMSE: 21087.0449\n",
            "Epoch 41\n",
            "Epoch 41, Loss: 59652885.95410156\n",
            "Test RMSE: 21069.9141\n",
            "Epoch 42\n",
            "Epoch 42, Loss: 59483098.52392578\n",
            "Test RMSE: 21052.8066\n",
            "Epoch 43\n",
            "Epoch 43, Loss: 59499743.330566406\n",
            "Test RMSE: 21035.7344\n",
            "Epoch 44\n",
            "Epoch 44, Loss: 59373425.735839844\n",
            "Test RMSE: 21018.6738\n",
            "Epoch 45\n",
            "Epoch 45, Loss: 59416765.92578125\n",
            "Test RMSE: 21001.6816\n",
            "Epoch 46\n",
            "Epoch 46, Loss: 59334398.38671875\n",
            "Test RMSE: 20984.6914\n",
            "Epoch 47\n",
            "Epoch 47, Loss: 59275686.2734375\n",
            "Test RMSE: 20967.7246\n",
            "Epoch 48\n",
            "Epoch 48, Loss: 59134258.149902344\n",
            "Test RMSE: 20950.8027\n",
            "Epoch 49\n",
            "Epoch 49, Loss: 59079624.3828125\n",
            "Test RMSE: 20933.9199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KFold CV on Column 1"
      ],
      "metadata": {
        "id": "ae2AVjrQJQ0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[df[\"cluster\"] == 1].drop('2024_price', axis=1)\n",
        "y = df[df[\"cluster\"] == 1]['2024_price']\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "X_numerical = X[numerical_cols].values\n",
        "X_combined = np.hstack((X[categorical_cols].values, X_numerical))\n",
        "\n",
        "\n",
        "kf = KFold(n_splits= 5, shuffle= True)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = lgb.LGBMRegressor(num_leaves= 50, n_estimators=50, learning_rate=0.5, random_state=42)\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# rmse = root_mean_squared_error(y_test, y_pred)\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "# print(f\"Root Mean Squared Error: {rmse}\")\n",
        "# print(f\"R-squared: {r2}\")\n",
        "\n",
        "\n",
        "scorer = make_scorer(mean_squared_error)\n",
        "\n",
        "scores = cross_val_score(model, X_combined, y, cv=kf, scoring=scorer, n_jobs=-1)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean RMSE: {np.sqrt(np.mean(scores)):.4f}, Std Dev: {np.std(scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOLkOH4EIdGe",
        "outputId": "c3a26d5d-3255-4d1c-88fe-d12bcb96c0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean RMSE: 5738.9168, Std Dev: 1572295.3991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uV3BXbnnJUaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}