# -*- coding: utf-8 -*-
"""Model Development.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17fsfVyDII1fldWup0jb7W_2BOmBQr3oo
"""

!pip install kagglehub
!pip install autogluon
!pip install dask

"""# Download the dataset from Kaggle"""

import kagglehub
import pandas as pd

# Download latest version
dataset = "syedanwarafridi/vehicle-sales-data"
path = kagglehub.dataset_download(dataset)
print(path)
dataset2 = "tsaustin/us-used-car-sales-data"
path2 = kagglehub.dataset_download(dataset2)
print("Second dataset downloaded to:", path2)

!mv /root/.cache/kagglehub/datasets /content/

"""# Data Preprocessing

## Drop Unnecessary Features
"""

df = pd.read_csv('/content/datasets/syedanwarafridi/vehicle-sales-data/versions/1/car_prices.csv')
df = df.drop(columns=['body', 'transmission', 'vin', 'seller'])
df.head()

# df2 = pd.read_csv('/content/datasets/tsaustin/us-used-car-sales-data/versions/4/used_car_sales.csv')
# df2 = df2.drop(columns=['Engine', 'BodyType', 'NumCylinders', 'DriveType', 'ID'])
# df2.rename(columns={
#     "pricesold": "sellingprice",
#     "Make": "make",
#     "Year": "year",
#     "Model": "model",
#     "Trim": "trim",
#     "Mileage": "odometer",
#     "yearsold" : "sale_year"
# }, inplace=True)
# df2.head()

"""## Handle Missing Data"""

def missing_summary(df):
    summary = {
        "Missing Values": df.isnull().sum(),
        "Missing Percentage (%)": (df.isnull().sum() / len(df)) * 100
    }
    return pd.DataFrame(summary)

df = df.replace([" ", "NA", "na", "--", "null"], pd.NA)
missing_summary(df)

# df2 = df2.replace([" ", "NA", "na", "--", "null", "NaN"], pd.NA)
# missing_summary(df2)

df = df.dropna(subset=['make', 'model', 'trim', 'color', 'interior', 'saledate'])
# df2 = df2.dropna(subset=['make', 'model', 'trim', 'sale_year', 'zipcode'])

"""## Fill missing numerical rows with median"""

df['condition'] = df['condition'].fillna(df['condition'].median())
df['odometer'] = df['odometer'].fillna(df['odometer'].median())
df['mmr'] = df['mmr'].fillna(df['mmr'].median())

missing_summary(df)

df.head()

"""## Merge Datasets

"""

# merged_df = pd.concat([df, df2], ignore_index=True)
# merged_df

"""# Inflation"""

import numpy as np
from multiprocessing import Pool, cpu_count

# Ensure saledate is a datetime object
df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')

# CPI data
cpi_data = {
    1980: 82.4, 1981: 90.9, 1982: 96.5, 1983: 99.6, 1984: 103.9, 1985: 107.6,
    1986: 109.6, 1987: 113.6, 1988: 118.3, 1989: 124.0, 1990: 130.7, 1991: 136.2,
    1992: 140.3, 1993: 144.5, 1994: 148.2, 1995: 152.4, 1996: 156.9, 1997: 160.5,
    1998: 163.0, 1999: 166.6, 2000: 172.2, 2001: 177.1, 2002: 179.9, 2003: 184.0,
    2004: 188.9, 2005: 195.3, 2006: 201.6, 2007: 207.3, 2008: 215.3, 2009: 214.5,
    2010: 218.1, 2011: 224.9, 2012: 229.6, 2013: 233.0, 2014: 236.7, 2015: 237.0,
    2016: 240.0, 2017: 245.1, 2018: 251.1, 2019: 255.7, 2020: 258.8, 2021: 270.9,
    2022: 292.0, 2023: 306.8, 2024: 315.6
}

# Convert CPI data to a DataFrame
cpi_df = pd.DataFrame(list(cpi_data.items()), columns=['cpi_year', 'cpi'])
cpi_df['inflation_factor'] = cpi_df['cpi'].apply(lambda x: cpi_data[2024] / x)

# Assign inflation factor for 2024 explicitly
cpi_df.loc[cpi_df['cpi_year'] == 2024, 'inflation_factor'] = 1.0
df['sale_year'] = df['saledate'].dt.year

# Ensure no duplicate columns exist before merging
if 'inflation_factor' in df.columns:
    df = df.drop(columns=['inflation_factor'])

# Merge CPI inflation factors into the main DataFrame
df = df.merge(cpi_df[['cpi_year', 'inflation_factor']], left_on='sale_year', right_on='cpi_year', how='left')
df = df.drop(columns=['cpi_year'])  # Drop the CPI year column

# missing_summary = pd.DataFrame({
#     "Missing Values": df.isnull().sum(),
#     "Missing Percentage (%)": (df.isnull().sum() / len(df)) * 100
# })
# print(missing_summary)

def generate_2024_rows(chunk):
    results = []
    for _, row in chunk.iterrows():
        # check if row is empty
        if pd.isna(row['sale_year']) or pd.isna(row['sellingprice']):
            continue

        # copy and set dates
        new_row = row.copy()
        new_row['saledate'] = pd.Timestamp('2024-01-01')
        new_row['sale_year'] = 2024

        # Adjust selling price using inflation factor
        new_row['sellingprice'] = row['sellingprice'] * (cpi_data[2024] / cpi_data.get(int(row['sale_year']), 1))
        new_row['inflation_factor'] = 1.0  # Inflation factor for 2024 should be 1
        results.append(new_row)

    return pd.DataFrame(results)

# Split the data into chunks for multiprocessing
def chunkify(dataframe, n_chunks):
    chunk_size = int(np.ceil(len(dataframe) / n_chunks))
    return [dataframe.iloc[i:i + chunk_size] for i in range(0, len(dataframe), chunk_size)]

# Use multiprocessing to generate 2024 rows
if __name__ == '__main__':
    num_cores = min(cpu_count(), 4)  # Use up to 4 cores
    chunks = chunkify(df, num_cores)  # Split data into chunks

    with Pool(num_cores) as pool:
        results = pool.map(generate_2024_rows, chunks)

    # Combine results from all processes
    generated_rows_df = pd.concat(results, ignore_index=True)

    # Combine the original dataset with the generated rows
    extended_df = pd.concat([df, generated_rows_df], ignore_index=True)

    extended_df['saledate'] = pd.to_datetime(extended_df['saledate']).dt.strftime('%m/%d/%Y')

    print(extended_df)

# cpi_data = {
#     1980: 82.4, 1981: 90.9, 1982: 96.5, 1983: 99.6, 1984: 103.9, 1985: 107.6,
#     1986: 109.6, 1987: 113.6, 1988: 118.3, 1989: 124.0, 1990: 130.7, 1991: 136.2,
#     1992: 140.3, 1993: 144.5, 1994: 148.2, 1995: 152.4, 1996: 156.9, 1997: 160.5,
#     1998: 163.0, 1999: 166.6, 2000: 172.2, 2001: 177.1, 2002: 179.9, 2003: 184.0,
#     2004: 188.9, 2005: 195.3, 2006: 201.6, 2007: 207.3, 2008: 215.3, 2009: 214.5,
#     2010: 218.1, 2011: 224.9, 2012: 229.6, 2013: 233.0, 2014: 236.7, 2015: 237.0,
#     2016: 240.0, 2017: 245.1, 2018: 251.1, 2019: 255.7, 2020: 258.8, 2021: 270.9,
#     2022: 292.0, 2023: 306.8, 2024: 315.6
# }

# # Convert CPI data to a DataFrame
# cpi_df = pd.DataFrame(list(cpi_data.items()), columns=['cpi_year', 'cpi'])
# cpi_df['inflation_factor'] = cpi_df['cpi'].apply(lambda x: cpi_data[2024] / x)

# # Assign inflation factor for 2024 explicitly
# cpi_df.loc[cpi_df['cpi_year'] == 2024, 'inflation_factor'] = 1.0

# # Extract the sale year from saledate
# # df1['sale_year'] = df1['saledate'].dt.year already done above

# # Ensure no duplicate columns exist before merging
# if 'inflation_factor' in df.columns:
#     df = df.drop(columns=['inflation_factor'])

# # Merge CPI inflation
# df = df.merge(cpi_df[['cpi_year', 'inflation_factor']], left_on='sale_year', right_on='cpi_year', how='left')
# df = df.drop(columns=['cpi_year'])  # Drop the CPI year

# # Step 1: Get unique combinations of year and trim
# unique_combinations = df[['year', 'trim']].drop_duplicates()

# # Step 2: Generate 2024 rows for unique combinations
# def generate_2024_rows(chunk):
#     results = []
#     for _, row in chunk.iterrows():
#         new_row = row.copy()
#         new_row['saledate'] = pd.Timestamp('2024-01-01')  # Set sale date to 2024
#         new_row['sale_year'] = 2024  # Set sale year to 2024
#         # Use the CPI formula for inflation adjustment
#         new_row['sellingprice'] = df.loc[
#             (df['year'] == row['year']) & (df['trim'] == row['trim']),
#             'sellingprice'
#         ].mean() * (cpi_data[2024] / cpi_data[row['year']])
#         new_row['inflation_factor'] = 1.0  # Inflation factor for 2024 is 1
#         results.append(new_row)
#     return pd.DataFrame(results)

# # Step 3: Process unique combinations with multiprocessing
# def chunkify(dataframe, n_chunks):
#     chunk_size = int(np.ceil(len(dataframe) / n_chunks))
#     return [dataframe.iloc[i:i + chunk_size] for i in range(0, len(dataframe), chunk_size)]

# if __name__ == '__main__':
#     num_cores = min(cpu_count(), 4)  # Use up to 4 cores
#     chunks = chunkify(unique_combinations, num_cores)  # Split unique combinations into chunks

#     with Pool(num_cores) as pool:
#         results = pool.map(generate_2024_rows, chunks)

#     # Combine results from all processes
#     generated_rows_df = pd.concat(results, ignore_index=True)

#     # Combine the generated rows
#     extended_df = pd.concat([df, generated_rows_df], ignore_index=True)

#     # Format the saledate
#     # extended_df['saledate'] = pd.to_datetime(extended_df['saledate']).dt.strftime('%m/%d/%Y')
#     print(extended_df)

extended_df.shape

"""# Model Selection

## So we decided to have a hybrid model using both clustering and prediction model

### Evaluation for clustering models based on literature review:

*   K-Means
*   DBSCAN
*   Agglomerative Clustering
*   Gaussian Mixture Model

### Evaluation metrics

*   Silhouette Score
*   Davies-Bouldin Index
*   Calinski-Harabasz Index

## Feature Selection
"""

from scipy.sparse import hstack
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import numpy as np

categorical_cols = ['make']
numerical_cols = ['year', 'condition', 'odometer', 'sellingprice']

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

X_numerical = df[numerical_cols].values
X_combined = np.hstack((df[categorical_cols].values, X_numerical))
print(X_combined[:10])

# Define clustering models
clustering_models = {
    "K-Means (k=3)": KMeans(n_clusters= 3, random_state= 42, n_init= 10),
    # "DBSCAN (eps=0.5, min_samples=5)": DBSCAN(eps=0.5, min_samples=10),
    # "Agglomerative Clustering (n_clusters=3)": AgglomerativeClustering(n_clusters=3),
}

# Store results
clustering_results = {}

for name, model in clustering_models.items():
    try:
        if isinstance(model, GaussianMixture):
            cluster_labels = model.fit_predict(X_combined)
        else:
            cluster_labels = model.fit(X_combined).labels_

        # Sample Data for Silhouette Score to Improve Speed
        if len(set(cluster_labels)) > 1:
            sample_size = min(10000, len(X_combined))  # Limit sample size
            silhouette = silhouette_score(X_combined[:sample_size], cluster_labels[:sample_size])
        else:
            silhouette = None

        # Faster Evaluation Metrics
        davies_bouldin = davies_bouldin_score(X_combined, cluster_labels)
        calinski_harabasz = calinski_harabasz_score(X_combined, cluster_labels)

        clustering_results[name] = {
            "Silhouette Score": silhouette,
            "Davies-Bouldin Index": davies_bouldin,
            "Calinski-Harabasz Index": calinski_harabasz
        }
    except Exception as e:
        clustering_results[name] = {"Error": str(e)}

# Convert Results to DataFrame
df_clustering_results = pd.DataFrame(clustering_results).T
df_clustering_results.index.name = "Model"
df_clustering_results

"""#Model Training by Cluster"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np



categorical_cols = ['make', 'model', 'trim', 'state', 'color', 'interior']
numerical_cols = ['year', 'condition', 'odometer', 'mmr', 'sellingprice']

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

X_numerical = df[numerical_cols].values
X_combined = np.hstack((df[categorical_cols].values, X_numerical))
X_combined = pd.DataFrame(X_combined, columns=categorical_cols + numerical_cols)

X_combined["cluster_label"] = cluster_labels

clusters = X_combined["cluster_label"].unique()
# Define features and target
target_col = "sellingprice"  # Target variable
features = categorical_cols + numerical_cols  # Adjust based on selected features

# Convert X_combined to DataFrame with column names
X_combined = pd.DataFrame(X_combined, columns=categorical_cols + numerical_cols)

# Add cluster labels
X_combined["cluster_label"] = cluster_labels

# Get unique cluster labels
clusters = X_combined["cluster_label"].unique()

# Store results
model_performance = {}

for cluster in clusters:
    print(f"\nTraining models for Cluster {cluster}...")

    # Filter data for the current cluster
    df_cluster = X_combined[X_combined["cluster_label"] == cluster]

    # Train-Test Split (80-20)
    X = df_cluster[features]
    y = df_cluster[target_col]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define models
    models = {
        "Linear Regression": LinearRegression(),
        "Support Vector Machine": SVR(kernel="rbf"),
        "LightGBM": lgb.LGBMRegressor(boosting_type="gbdt", objective="regression", random_state=42)
    }

    cluster_results = {}

    for model_name, model in models.items():
        print(f" - Training {model_name} for Cluster {cluster}...")

        # Train model
        model.fit(X_train, y_train)

        # Predictions
        y_pred = model.predict(X_test)

        # Evaluate performance
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)

        # Store results
        cluster_results[model_name] = {"RMSE": rmse, "R²": r2}

    model_performance[f"Cluster {cluster}"] = cluster_results

# Convert results to DataFrame for easy comparison
df_model_performance = pd.DataFrame(model_performance).T
df_model_performance